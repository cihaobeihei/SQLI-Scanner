2015-04-25 17:45:26+0800 [scrapy] INFO: Scrapy 0.24.5 started (bot: scrapybot)
2015-04-25 17:45:26+0800 [scrapy] INFO: Optional features available: ssl, http11
2015-04-25 17:45:26+0800 [scrapy] INFO: Overridden settings: {'DEFAULT_ITEM_CLASS': 'dirbot.items.Website', 'NEWSPIDER_MODULE': 'dirbot.spiders', 'SPIDER_MODULES': ['dirbot.spiders']}
2015-04-25 17:45:26+0800 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-04-25 17:45:26+0800 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-04-25 17:45:26+0800 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-04-25 17:45:26+0800 [scrapy] INFO: Enabled item pipelines: FilterWordsPipeline
2015-04-25 17:45:26+0800 [step1] INFO: Spider opened
2015-04-25 17:45:26+0800 [step1] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-04-25 17:45:26+0800 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6025
2015-04-25 17:45:26+0800 [scrapy] DEBUG: Web service listening on 127.0.0.1:6082
2015-04-25 17:45:26+0800 [step1] DEBUG: Crawled (200) <GET http://bm1.com/bm1/login.php> (referer: None)
2015-04-25 17:45:26+0800 [step1] DEBUG: Crawled (200) <GET http://bm1.com/bm1/login.php> (referer: http://bm1.com/bm1/login.php)
2015-04-25 17:45:26+0800 [step1] DEBUG: Crawled (404) <GET http://bm1.com/bm1/formhash(this.form,%20this.form.password)> (referer: http://bm1.com/bm1/login.php)
2015-04-25 17:45:26+0800 [step1] DEBUG: Ignoring response <404 http://bm1.com/bm1/formhash(this.form,%20this.form.password)>: HTTP status code is not handled or not allowed
2015-04-25 17:45:26+0800 [step1] DEBUG: Crawled (200) <GET http://bm1.com/bm1/register.php> (referer: http://bm1.com/bm1/login.php)
2015-04-25 17:45:26+0800 [step1] DEBUG: Crawled (404) <GET http://bm1.com/bm1/return%20regformhash(this.form,%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20this.form.username,%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20this.form.email,%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20this.form.password,%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20this.form.confirmpwd)> (referer: http://bm1.com/bm1/register.php)
2015-04-25 17:45:26+0800 [step1] DEBUG: Ignoring response <404 http://bm1.com/bm1/return%20regformhash(this.form,%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20this.form.username,%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20this.form.email,%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20this.form.password,%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20this.form.confirmpwd)>: HTTP status code is not handled or not allowed
2015-04-25 17:45:26+0800 [step1] DEBUG: Crawled (200) <GET http://bm1.com/bm1/index.php> (referer: http://bm1.com/bm1/register.php)
2015-04-25 17:45:26+0800 [step1] DEBUG: Crawled (200) <GET http://bm1.com/bm1/login.php> (referer: http://bm1.com/bm1/index.php)
2015-04-25 17:45:26+0800 [step1] INFO: Closing spider (finished)
2015-04-25 17:45:26+0800 [step1] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 2655,
	 'downloader/request_count': 7,
	 'downloader/request_method_count/GET': 7,
	 'downloader/response_bytes': 4983,
	 'downloader/response_count': 7,
	 'downloader/response_status_count/200': 5,
	 'downloader/response_status_count/404': 2,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 4, 25, 9, 45, 26, 641669),
	 'log_count/DEBUG': 11,
	 'log_count/INFO': 7,
	 'request_depth_max': 4,
	 'response_received_count': 7,
	 'scheduler/dequeued': 7,
	 'scheduler/dequeued/memory': 7,
	 'scheduler/enqueued': 7,
	 'scheduler/enqueued/memory': 7,
	 'start_time': datetime.datetime(2015, 4, 25, 9, 45, 26, 598295)}
2015-04-25 17:45:26+0800 [step1] INFO: Spider closed (finished)
2015-04-25 17:45:26+0800 [scrapy] INFO: Scrapy 0.24.5 started (bot: scrapybot)
2015-04-25 17:45:26+0800 [scrapy] INFO: Optional features available: ssl, http11
2015-04-25 17:45:26+0800 [scrapy] INFO: Overridden settings: {'DEFAULT_ITEM_CLASS': 'dirbot.items.Website', 'NEWSPIDER_MODULE': 'dirbot.spiders', 'SPIDER_MODULES': ['dirbot.spiders']}
2015-04-25 17:45:27+0800 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-04-25 17:45:27+0800 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-04-25 17:45:27+0800 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-04-25 17:45:27+0800 [scrapy] INFO: Enabled item pipelines: FilterWordsPipeline
2015-04-25 17:45:27+0800 [step1login] INFO: Spider opened
2015-04-25 17:45:27+0800 [step1login] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-04-25 17:45:27+0800 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6025
2015-04-25 17:45:27+0800 [scrapy] DEBUG: Web service listening on 127.0.0.1:6082
2015-04-25 17:45:27+0800 [step1login] DEBUG: Crawled (200) <GET http://bm1.com/bm1/login.php> (referer: None)
2015-04-25 17:45:27+0800 [step1login] DEBUG: Crawled (200) <POST http://bm1.com/bm1/login.php> (referer: http://bm1.com/bm1/login.php)
2015-04-25 17:45:27+0800 [step1login] ERROR: Login failed
2015-04-25 17:45:27+0800 [step1login] INFO: Closing spider (finished)
2015-04-25 17:45:27+0800 [step1login] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 618,
	 'downloader/request_count': 2,
	 'downloader/request_method_count/GET': 1,
	 'downloader/request_method_count/POST': 1,
	 'downloader/response_bytes': 1713,
	 'downloader/response_count': 2,
	 'downloader/response_status_count/200': 2,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 4, 25, 9, 45, 27, 64646),
	 'log_count/DEBUG': 4,
	 'log_count/ERROR': 1,
	 'log_count/INFO': 7,
	 'request_depth_max': 1,
	 'response_received_count': 2,
	 'scheduler/dequeued': 2,
	 'scheduler/dequeued/memory': 2,
	 'scheduler/enqueued': 2,
	 'scheduler/enqueued/memory': 2,
	 'start_time': datetime.datetime(2015, 4, 25, 9, 45, 27, 48753)}
2015-04-25 17:45:27+0800 [step1login] INFO: Spider closed (finished)
2015-04-25 17:45:27+0800 [scrapy] INFO: Scrapy 0.24.5 started (bot: scrapybot)
2015-04-25 17:45:27+0800 [scrapy] INFO: Optional features available: ssl, http11
2015-04-25 17:45:27+0800 [scrapy] INFO: Overridden settings: {'DEFAULT_ITEM_CLASS': 'dirbot.items.Website', 'NEWSPIDER_MODULE': 'dirbot.spiders', 'SPIDER_MODULES': ['dirbot.spiders']}
2015-04-25 17:45:27+0800 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-04-25 17:45:27+0800 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-04-25 17:45:27+0800 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-04-25 17:45:27+0800 [scrapy] INFO: Enabled item pipelines: FilterWordsPipeline
2015-04-25 17:45:27+0800 [step3login] INFO: Spider opened
2015-04-25 17:45:27+0800 [step3login] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-04-25 17:45:27+0800 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6025
2015-04-25 17:45:27+0800 [scrapy] DEBUG: Web service listening on 127.0.0.1:6082
2015-04-25 17:45:27+0800 [step3login] DEBUG: Crawled (200) <GET http://bm1.com/bm1/login.php> (referer: None)
2015-04-25 17:45:27+0800 [step3login] DEBUG: Crawled (200) <POST http://bm1.com/bm1/includes/process_login.php> (referer: http://bm1.com/bm1/login.php)
2015-04-25 17:45:27+0800 [step3login] INFO: Closing spider (finished)
2015-04-25 17:45:27+0800 [step3login] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 641,
	 'downloader/request_count': 2,
	 'downloader/request_method_count/GET': 1,
	 'downloader/request_method_count/POST': 1,
	 'downloader/response_bytes': 1269,
	 'downloader/response_count': 2,
	 'downloader/response_status_count/200': 2,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 4, 25, 9, 45, 27, 482599),
	 'log_count/DEBUG': 4,
	 'log_count/INFO': 7,
	 'request_depth_max': 1,
	 'response_received_count': 2,
	 'scheduler/dequeued': 2,
	 'scheduler/dequeued/memory': 2,
	 'scheduler/enqueued': 2,
	 'scheduler/enqueued/memory': 2,
	 'start_time': datetime.datetime(2015, 4, 25, 9, 45, 27, 466636)}
2015-04-25 17:45:27+0800 [step3login] INFO: Spider closed (finished)
2015-04-25 17:45:27+0800 [scrapy] INFO: Scrapy 0.24.5 started (bot: scrapybot)
2015-04-25 17:45:27+0800 [scrapy] INFO: Optional features available: ssl, http11
2015-04-25 17:45:27+0800 [scrapy] INFO: Overridden settings: {'DEFAULT_ITEM_CLASS': 'dirbot.items.Website', 'NEWSPIDER_MODULE': 'dirbot.spiders', 'SPIDER_MODULES': ['dirbot.spiders']}
2015-04-25 17:45:27+0800 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-04-25 17:45:27+0800 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-04-25 17:45:27+0800 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-04-25 17:45:27+0800 [scrapy] INFO: Enabled item pipelines: FilterWordsPipeline
2015-04-25 17:45:27+0800 [step3] INFO: Spider opened
2015-04-25 17:45:27+0800 [step3] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-04-25 17:45:27+0800 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6025
2015-04-25 17:45:27+0800 [scrapy] DEBUG: Web service listening on 127.0.0.1:6082
2015-04-25 17:45:27+0800 [step3] DEBUG: Crawled (200) <GET http://bm1.com/bm1/login.php> (referer: None)
2015-04-25 17:45:27+0800 [step3] DEBUG: Crawled (200) <POST http://bm1.com/bm1/includes/process_login.php> (referer: http://bm1.com/bm1/login.php)
2015-04-25 17:45:27+0800 [step3] DEBUG: Crawled (200) <GET http://bm1.com/bm1/login.php> (referer: http://bm1.com/bm1/includes/process_login.php)
2015-04-25 17:45:27+0800 [step3] DEBUG: Crawled (404) <GET http://bm1.com/bm1/return%20regformhash(this.form,%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20this.form.username,%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20this.form.email,%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20this.form.password,%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20this.form.confirmpwd)> (referer: http://bm1.com/bm1/includes/process_login.php)
2015-04-25 17:45:27+0800 [step3] DEBUG: Crawled (200) <GET http://bm1.com/bm1/index.php> (referer: http://bm1.com/bm1/includes/process_login.php)
2015-04-25 17:45:27+0800 [step3] DEBUG: Crawled (404) <GET http://bm1.com/bm1/formhash(this.form,%20this.form.password)> (referer: http://bm1.com/bm1/includes/process_login.php)
2015-04-25 17:45:27+0800 [step3] DEBUG: Crawled (200) <GET http://bm1.com/bm1/register.php> (referer: http://bm1.com/bm1/includes/process_login.php)
2015-04-25 17:45:27+0800 [step3] DEBUG: Ignoring response <404 http://bm1.com/bm1/return%20regformhash(this.form,%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20this.form.username,%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20this.form.email,%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20this.form.password,%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20this.form.confirmpwd)>: HTTP status code is not handled or not allowed
2015-04-25 17:45:27+0800 [step3] DEBUG: Ignoring response <404 http://bm1.com/bm1/formhash(this.form,%20this.form.password)>: HTTP status code is not handled or not allowed
2015-04-25 17:45:27+0800 [step3] DEBUG: Crawled (404) <GET http://bm1.com/bm1/login.php+AND+SEELCT> (referer: http://bm1.com/bm1/login.php)
2015-04-25 17:45:27+0800 [step3] DEBUG: Crawled (404) <GET http://bm1.com/bm1/login.php'kasdgh> (referer: http://bm1.com/bm1/login.php)
2015-04-25 17:45:27+0800 [step3] DEBUG: Crawled (404) <GET http://bm1.com/bm1/login.php+and+SLAP(10)+--+> (referer: http://bm1.com/bm1/login.php)
2015-04-25 17:45:27+0800 [step3] DEBUG: Ignoring response <404 http://bm1.com/bm1/login.php+AND+SEELCT>: HTTP status code is not handled or not allowed
2015-04-25 17:45:27+0800 [step3] DEBUG: Ignoring response <404 http://bm1.com/bm1/login.php'kasdgh>: HTTP status code is not handled or not allowed
2015-04-25 17:45:27+0800 [step3] DEBUG: Ignoring response <404 http://bm1.com/bm1/login.php+and+SLAP(10)+--+>: HTTP status code is not handled or not allowed
2015-04-25 17:45:27+0800 [step3] DEBUG: Crawled (404) <GET http://bm1.com/bm1/register.php+AND+SEELCT> (referer: http://bm1.com/bm1/register.php)
2015-04-25 17:45:27+0800 [step3] DEBUG: Crawled (404) <GET http://bm1.com/bm1/register.php'kasdgh> (referer: http://bm1.com/bm1/register.php)
2015-04-25 17:45:27+0800 [step3] DEBUG: Crawled (404) <GET http://bm1.com/bm1/register.php+and+SLAP(10)+--+> (referer: http://bm1.com/bm1/register.php)
2015-04-25 17:45:27+0800 [step3] DEBUG: Crawled (404) <GET http://bm1.com/bm1/index.php+AND+SEELCT> (referer: http://bm1.com/bm1/index.php)
2015-04-25 17:45:27+0800 [step3] DEBUG: Crawled (404) <GET http://bm1.com/bm1/index.php'kasdgh> (referer: http://bm1.com/bm1/index.php)
2015-04-25 17:45:27+0800 [step3] DEBUG: Ignoring response <404 http://bm1.com/bm1/register.php+AND+SEELCT>: HTTP status code is not handled or not allowed
2015-04-25 17:45:27+0800 [step3] DEBUG: Ignoring response <404 http://bm1.com/bm1/register.php'kasdgh>: HTTP status code is not handled or not allowed
2015-04-25 17:45:27+0800 [step3] DEBUG: Ignoring response <404 http://bm1.com/bm1/register.php+and+SLAP(10)+--+>: HTTP status code is not handled or not allowed
2015-04-25 17:45:27+0800 [step3] DEBUG: Ignoring response <404 http://bm1.com/bm1/index.php+AND+SEELCT>: HTTP status code is not handled or not allowed
2015-04-25 17:45:27+0800 [step3] DEBUG: Ignoring response <404 http://bm1.com/bm1/index.php'kasdgh>: HTTP status code is not handled or not allowed
2015-04-25 17:45:27+0800 [step3] DEBUG: Crawled (404) <GET http://bm1.com/bm1/index.php+and+SLAP(10)+--+> (referer: http://bm1.com/bm1/index.php)
2015-04-25 17:45:27+0800 [step3] DEBUG: Ignoring response <404 http://bm1.com/bm1/index.php+and+SLAP(10)+--+>: HTTP status code is not handled or not allowed
2015-04-25 17:45:27+0800 [step3] INFO: Closing spider (finished)
2015-04-25 17:45:27+0800 [step3] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 5761,
	 'downloader/request_count': 16,
	 'downloader/request_method_count/GET': 15,
	 'downloader/request_method_count/POST': 1,
	 'downloader/response_bytes': 8451,
	 'downloader/response_count': 16,
	 'downloader/response_status_count/200': 5,
	 'downloader/response_status_count/404': 11,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 4, 25, 9, 45, 27, 962967),
	 'log_count/DEBUG': 29,
	 'log_count/INFO': 7,
	 'request_depth_max': 3,
	 'response_received_count': 16,
	 'scheduler/dequeued': 16,
	 'scheduler/dequeued/memory': 16,
	 'scheduler/enqueued': 16,
	 'scheduler/enqueued/memory': 16,
	 'start_time': datetime.datetime(2015, 4, 25, 9, 45, 27, 870268)}
2015-04-25 17:45:27+0800 [step3] INFO: Spider closed (finished)
2015-04-25 17:45:28+0800 [scrapy] INFO: Scrapy 0.24.5 started (bot: scrapybot)
2015-04-25 17:45:28+0800 [scrapy] INFO: Optional features available: ssl, http11
2015-04-25 17:45:28+0800 [scrapy] INFO: Overridden settings: {'DEFAULT_ITEM_CLASS': 'dirbot.items.Website', 'NEWSPIDER_MODULE': 'dirbot.spiders', 'SPIDER_MODULES': ['dirbot.spiders']}
2015-04-25 17:45:28+0800 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-04-25 17:45:28+0800 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-04-25 17:45:28+0800 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-04-25 17:45:28+0800 [scrapy] INFO: Enabled item pipelines: FilterWordsPipeline
2015-04-25 17:45:28+0800 [step1] INFO: Spider opened
2015-04-25 17:45:28+0800 [step1] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-04-25 17:45:28+0800 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6025
2015-04-25 17:45:28+0800 [scrapy] DEBUG: Web service listening on 127.0.0.1:6082
2015-04-25 17:45:28+0800 [step1] DEBUG: Crawled (200) <GET http://bm3.com/bm3.php> (referer: None)
2015-04-25 17:45:28+0800 [step1] DEBUG: Crawled (200) <GET http://bm3.com/bm3.php> (referer: http://bm3.com/bm3.php)
2015-04-25 17:45:28+0800 [step1] INFO: Closing spider (finished)
2015-04-25 17:45:28+0800 [step1] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 505,
	 'downloader/request_count': 2,
	 'downloader/request_method_count/GET': 2,
	 'downloader/response_bytes': 1294,
	 'downloader/response_count': 2,
	 'downloader/response_status_count/200': 2,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 4, 25, 9, 45, 28, 393251),
	 'log_count/DEBUG': 4,
	 'log_count/INFO': 7,
	 'request_depth_max': 1,
	 'response_received_count': 2,
	 'scheduler/dequeued': 2,
	 'scheduler/dequeued/memory': 2,
	 'scheduler/enqueued': 2,
	 'scheduler/enqueued/memory': 2,
	 'start_time': datetime.datetime(2015, 4, 25, 9, 45, 28, 373491)}
2015-04-25 17:45:28+0800 [step1] INFO: Spider closed (finished)
2015-04-25 17:45:28+0800 [scrapy] INFO: Scrapy 0.24.5 started (bot: scrapybot)
2015-04-25 17:45:28+0800 [scrapy] INFO: Optional features available: ssl, http11
2015-04-25 17:45:28+0800 [scrapy] INFO: Overridden settings: {'DEFAULT_ITEM_CLASS': 'dirbot.items.Website', 'NEWSPIDER_MODULE': 'dirbot.spiders', 'SPIDER_MODULES': ['dirbot.spiders']}
2015-04-25 17:45:28+0800 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-04-25 17:45:28+0800 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-04-25 17:45:28+0800 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-04-25 17:45:28+0800 [scrapy] INFO: Enabled item pipelines: FilterWordsPipeline
2015-04-25 17:45:28+0800 [step1login] INFO: Spider opened
2015-04-25 17:45:28+0800 [step1login] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-04-25 17:45:28+0800 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6025
2015-04-25 17:45:28+0800 [scrapy] DEBUG: Web service listening on 127.0.0.1:6082
2015-04-25 17:45:28+0800 [step1login] DEBUG: Crawled (200) <GET http://bm3.com/bm3.php> (referer: None)
2015-04-25 17:45:28+0800 [step1login] ERROR: Spider error processing <GET http://bm3.com/bm3.php>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 26, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/student/Desktop/Code/SqliScanner/Assignment3/dirbot/spiders/step1_login.py", line 35, in parse
	    args, url, method, uid, pid = fill_login_form(response.url, response.body, self.login_user, self.login_pass)
	  File "/home/student/Desktop/Code/SqliScanner/Assignment3/dirbot/spiders/loginform.py", line 80, in fill_login_form
	    form.fields[userfield] = username
	  File "/usr/lib/python2.7/dist-packages/lxml/html/__init__.py", line 960, in __setitem__
	    self.inputs[item].value = value
	  File "/usr/lib/python2.7/dist-packages/lxml/html/__init__.py", line 1025, in __getitem__
	    "No input element with the name %r" % name)
	exceptions.KeyError: 'No input element with the name None'
	
2015-04-25 17:45:28+0800 [step1login] INFO: Closing spider (finished)
2015-04-25 17:45:28+0800 [step1login] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 213,
	 'downloader/request_count': 1,
	 'downloader/request_method_count/GET': 1,
	 'downloader/response_bytes': 676,
	 'downloader/response_count': 1,
	 'downloader/response_status_count/200': 1,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 4, 25, 9, 45, 28, 809490),
	 'log_count/DEBUG': 3,
	 'log_count/ERROR': 1,
	 'log_count/INFO': 7,
	 'response_received_count': 1,
	 'scheduler/dequeued': 1,
	 'scheduler/dequeued/memory': 1,
	 'scheduler/enqueued': 1,
	 'scheduler/enqueued/memory': 1,
	 'spider_exceptions/KeyError': 1,
	 'start_time': datetime.datetime(2015, 4, 25, 9, 45, 28, 796148)}
2015-04-25 17:45:28+0800 [step1login] INFO: Spider closed (finished)
2015-04-25 17:45:29+0800 [scrapy] INFO: Scrapy 0.24.5 started (bot: scrapybot)
2015-04-25 17:45:29+0800 [scrapy] INFO: Optional features available: ssl, http11
2015-04-25 17:45:29+0800 [scrapy] INFO: Overridden settings: {'DEFAULT_ITEM_CLASS': 'dirbot.items.Website', 'NEWSPIDER_MODULE': 'dirbot.spiders', 'SPIDER_MODULES': ['dirbot.spiders']}
2015-04-25 17:45:29+0800 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-04-25 17:45:29+0800 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-04-25 17:45:29+0800 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-04-25 17:45:29+0800 [scrapy] INFO: Enabled item pipelines: FilterWordsPipeline
2015-04-25 17:45:29+0800 [step3login] INFO: Spider opened
2015-04-25 17:45:29+0800 [step3login] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-04-25 17:45:29+0800 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6025
2015-04-25 17:45:29+0800 [scrapy] DEBUG: Web service listening on 127.0.0.1:6082
2015-04-25 17:45:29+0800 [step3login] DEBUG: Crawled (200) <GET http://bm3.com/bm3.php> (referer: None)
2015-04-25 17:45:29+0800 [step3login] DEBUG: nothing to do here
2015-04-25 17:45:29+0800 [step3login] INFO: Closing spider (finished)
2015-04-25 17:45:29+0800 [step3login] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 213,
	 'downloader/request_count': 1,
	 'downloader/request_method_count/GET': 1,
	 'downloader/response_bytes': 676,
	 'downloader/response_count': 1,
	 'downloader/response_status_count/200': 1,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 4, 25, 9, 45, 29, 212505),
	 'log_count/DEBUG': 4,
	 'log_count/INFO': 7,
	 'response_received_count': 1,
	 'scheduler/dequeued': 1,
	 'scheduler/dequeued/memory': 1,
	 'scheduler/enqueued': 1,
	 'scheduler/enqueued/memory': 1,
	 'start_time': datetime.datetime(2015, 4, 25, 9, 45, 29, 201328)}
2015-04-25 17:45:29+0800 [step3login] INFO: Spider closed (finished)
2015-04-25 17:45:29+0800 [scrapy] INFO: Scrapy 0.24.5 started (bot: scrapybot)
2015-04-25 17:45:29+0800 [scrapy] INFO: Optional features available: ssl, http11
2015-04-25 17:45:29+0800 [scrapy] INFO: Overridden settings: {'DEFAULT_ITEM_CLASS': 'dirbot.items.Website', 'NEWSPIDER_MODULE': 'dirbot.spiders', 'SPIDER_MODULES': ['dirbot.spiders']}
2015-04-25 17:45:29+0800 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-04-25 17:45:29+0800 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-04-25 17:45:29+0800 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-04-25 17:45:29+0800 [scrapy] INFO: Enabled item pipelines: FilterWordsPipeline
2015-04-25 17:45:29+0800 [step3] INFO: Spider opened
2015-04-25 17:45:29+0800 [step3] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-04-25 17:45:29+0800 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6025
2015-04-25 17:45:29+0800 [scrapy] DEBUG: Web service listening on 127.0.0.1:6082
2015-04-25 17:45:29+0800 [step3] DEBUG: Crawled (200) <GET http://bm3.com/bm3.php> (referer: None)
2015-04-25 17:45:29+0800 [step3] ERROR: Spider error processing <GET http://bm3.com/bm3.php>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 26, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/student/Desktop/Code/SqliScanner/Assignment3/dirbot/spiders/step3.py", line 130, in parse
	    print "urlList",urlList
	exceptions.NameError: global name 'urlList' is not defined
	
2015-04-25 17:45:29+0800 [step3] INFO: Closing spider (finished)
2015-04-25 17:45:29+0800 [step3] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 213,
	 'downloader/request_count': 1,
	 'downloader/request_method_count/GET': 1,
	 'downloader/response_bytes': 676,
	 'downloader/response_count': 1,
	 'downloader/response_status_count/200': 1,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 4, 25, 9, 45, 29, 625390),
	 'log_count/DEBUG': 3,
	 'log_count/ERROR': 1,
	 'log_count/INFO': 7,
	 'response_received_count': 1,
	 'scheduler/dequeued': 1,
	 'scheduler/dequeued/memory': 1,
	 'scheduler/enqueued': 1,
	 'scheduler/enqueued/memory': 1,
	 'spider_exceptions/NameError': 1,
	 'start_time': datetime.datetime(2015, 4, 25, 9, 45, 29, 614268)}
2015-04-25 17:45:29+0800 [step3] INFO: Spider closed (finished)
2015-04-25 17:45:29+0800 [scrapy] INFO: Scrapy 0.24.5 started (bot: scrapybot)
2015-04-25 17:45:29+0800 [scrapy] INFO: Optional features available: ssl, http11
2015-04-25 17:45:29+0800 [scrapy] INFO: Overridden settings: {'DEFAULT_ITEM_CLASS': 'dirbot.items.Website', 'NEWSPIDER_MODULE': 'dirbot.spiders', 'SPIDER_MODULES': ['dirbot.spiders']}
2015-04-25 17:45:30+0800 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-04-25 17:45:30+0800 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-04-25 17:45:30+0800 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-04-25 17:45:30+0800 [scrapy] INFO: Enabled item pipelines: FilterWordsPipeline
2015-04-25 17:45:30+0800 [step1] INFO: Spider opened
2015-04-25 17:45:30+0800 [step1] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-04-25 17:45:30+0800 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6025
2015-04-25 17:45:30+0800 [scrapy] DEBUG: Web service listening on 127.0.0.1:6082
2015-04-25 17:45:30+0800 [step1] DEBUG: Crawled (200) <GET http://bm5.com/bm5.php> (referer: None)
2015-04-25 17:45:30+0800 [step1] DEBUG: Crawled (200) <GET http://bm5.com/bm5.php> (referer: http://bm5.com/bm5.php)
2015-04-25 17:45:30+0800 [step1] INFO: Closing spider (finished)
2015-04-25 17:45:30+0800 [step1] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 505,
	 'downloader/request_count': 2,
	 'downloader/request_method_count/GET': 2,
	 'downloader/response_bytes': 1260,
	 'downloader/response_count': 2,
	 'downloader/response_status_count/200': 2,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 4, 25, 9, 45, 30, 65481),
	 'log_count/DEBUG': 4,
	 'log_count/INFO': 7,
	 'request_depth_max': 1,
	 'response_received_count': 2,
	 'scheduler/dequeued': 2,
	 'scheduler/dequeued/memory': 2,
	 'scheduler/enqueued': 2,
	 'scheduler/enqueued/memory': 2,
	 'start_time': datetime.datetime(2015, 4, 25, 9, 45, 30, 47872)}
2015-04-25 17:45:30+0800 [step1] INFO: Spider closed (finished)
2015-04-25 17:45:30+0800 [scrapy] INFO: Scrapy 0.24.5 started (bot: scrapybot)
2015-04-25 17:45:30+0800 [scrapy] INFO: Optional features available: ssl, http11
2015-04-25 17:45:30+0800 [scrapy] INFO: Overridden settings: {'DEFAULT_ITEM_CLASS': 'dirbot.items.Website', 'NEWSPIDER_MODULE': 'dirbot.spiders', 'SPIDER_MODULES': ['dirbot.spiders']}
2015-04-25 17:45:30+0800 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-04-25 17:45:30+0800 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-04-25 17:45:30+0800 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-04-25 17:45:30+0800 [scrapy] INFO: Enabled item pipelines: FilterWordsPipeline
2015-04-25 17:45:30+0800 [step1login] INFO: Spider opened
2015-04-25 17:45:30+0800 [step1login] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-04-25 17:45:30+0800 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6025
2015-04-25 17:45:30+0800 [scrapy] DEBUG: Web service listening on 127.0.0.1:6082
2015-04-25 17:45:30+0800 [step1login] DEBUG: Crawled (200) <GET http://bm5.com/bm5.php> (referer: None)
2015-04-25 17:45:30+0800 [step1login] ERROR: Spider error processing <GET http://bm5.com/bm5.php>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 26, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/student/Desktop/Code/SqliScanner/Assignment3/dirbot/spiders/step1_login.py", line 35, in parse
	    args, url, method, uid, pid = fill_login_form(response.url, response.body, self.login_user, self.login_pass)
	  File "/home/student/Desktop/Code/SqliScanner/Assignment3/dirbot/spiders/loginform.py", line 81, in fill_login_form
	    form.fields[passfield] = password
	  File "/usr/lib/python2.7/dist-packages/lxml/html/__init__.py", line 960, in __setitem__
	    self.inputs[item].value = value
	  File "/usr/lib/python2.7/dist-packages/lxml/html/__init__.py", line 1025, in __getitem__
	    "No input element with the name %r" % name)
	exceptions.KeyError: 'No input element with the name None'
	
2015-04-25 17:45:30+0800 [step1login] INFO: Closing spider (finished)
2015-04-25 17:45:30+0800 [step1login] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 213,
	 'downloader/request_count': 1,
	 'downloader/request_method_count/GET': 1,
	 'downloader/response_bytes': 659,
	 'downloader/response_count': 1,
	 'downloader/response_status_count/200': 1,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 4, 25, 9, 45, 30, 467747),
	 'log_count/DEBUG': 3,
	 'log_count/ERROR': 1,
	 'log_count/INFO': 7,
	 'response_received_count': 1,
	 'scheduler/dequeued': 1,
	 'scheduler/dequeued/memory': 1,
	 'scheduler/enqueued': 1,
	 'scheduler/enqueued/memory': 1,
	 'spider_exceptions/KeyError': 1,
	 'start_time': datetime.datetime(2015, 4, 25, 9, 45, 30, 455453)}
2015-04-25 17:45:30+0800 [step1login] INFO: Spider closed (finished)
2015-04-25 17:45:30+0800 [scrapy] INFO: Scrapy 0.24.5 started (bot: scrapybot)
2015-04-25 17:45:30+0800 [scrapy] INFO: Optional features available: ssl, http11
2015-04-25 17:45:30+0800 [scrapy] INFO: Overridden settings: {'DEFAULT_ITEM_CLASS': 'dirbot.items.Website', 'NEWSPIDER_MODULE': 'dirbot.spiders', 'SPIDER_MODULES': ['dirbot.spiders']}
2015-04-25 17:45:30+0800 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-04-25 17:45:30+0800 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-04-25 17:45:30+0800 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-04-25 17:45:30+0800 [scrapy] INFO: Enabled item pipelines: FilterWordsPipeline
2015-04-25 17:45:30+0800 [step3login] INFO: Spider opened
2015-04-25 17:45:30+0800 [step3login] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-04-25 17:45:30+0800 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6025
2015-04-25 17:45:30+0800 [scrapy] DEBUG: Web service listening on 127.0.0.1:6082
2015-04-25 17:45:30+0800 [step3login] DEBUG: Crawled (200) <GET http://bm5.com/bm5.php> (referer: None)
2015-04-25 17:45:30+0800 [step3login] DEBUG: nothing to do here
2015-04-25 17:45:30+0800 [step3login] INFO: Closing spider (finished)
2015-04-25 17:45:30+0800 [step3login] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 213,
	 'downloader/request_count': 1,
	 'downloader/request_method_count/GET': 1,
	 'downloader/response_bytes': 659,
	 'downloader/response_count': 1,
	 'downloader/response_status_count/200': 1,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 4, 25, 9, 45, 30, 899123),
	 'log_count/DEBUG': 4,
	 'log_count/INFO': 7,
	 'response_received_count': 1,
	 'scheduler/dequeued': 1,
	 'scheduler/dequeued/memory': 1,
	 'scheduler/enqueued': 1,
	 'scheduler/enqueued/memory': 1,
	 'start_time': datetime.datetime(2015, 4, 25, 9, 45, 30, 888609)}
2015-04-25 17:45:30+0800 [step3login] INFO: Spider closed (finished)
2015-04-25 17:45:31+0800 [scrapy] INFO: Scrapy 0.24.5 started (bot: scrapybot)
2015-04-25 17:45:31+0800 [scrapy] INFO: Optional features available: ssl, http11
2015-04-25 17:45:31+0800 [scrapy] INFO: Overridden settings: {'DEFAULT_ITEM_CLASS': 'dirbot.items.Website', 'NEWSPIDER_MODULE': 'dirbot.spiders', 'SPIDER_MODULES': ['dirbot.spiders']}
2015-04-25 17:45:31+0800 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-04-25 17:45:31+0800 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-04-25 17:45:31+0800 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-04-25 17:45:31+0800 [scrapy] INFO: Enabled item pipelines: FilterWordsPipeline
2015-04-25 17:45:31+0800 [step3] INFO: Spider opened
2015-04-25 17:45:31+0800 [step3] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-04-25 17:45:31+0800 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6025
2015-04-25 17:45:31+0800 [scrapy] DEBUG: Web service listening on 127.0.0.1:6082
2015-04-25 17:45:31+0800 [step3] DEBUG: Crawled (200) <GET http://bm5.com/bm5.php> (referer: None)
2015-04-25 17:45:31+0800 [step3] INFO: Closing spider (finished)
2015-04-25 17:45:31+0800 [step3] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 213,
	 'downloader/request_count': 1,
	 'downloader/request_method_count/GET': 1,
	 'downloader/response_bytes': 659,
	 'downloader/response_count': 1,
	 'downloader/response_status_count/200': 1,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 4, 25, 9, 45, 31, 303519),
	 'log_count/DEBUG': 3,
	 'log_count/INFO': 7,
	 'response_received_count': 1,
	 'scheduler/dequeued': 1,
	 'scheduler/dequeued/memory': 1,
	 'scheduler/enqueued': 1,
	 'scheduler/enqueued/memory': 1,
	 'start_time': datetime.datetime(2015, 4, 25, 9, 45, 31, 293152)}
2015-04-25 17:45:31+0800 [step3] INFO: Spider closed (finished)
2015-04-25 17:45:31+0800 [scrapy] INFO: Scrapy 0.24.5 started (bot: scrapybot)
2015-04-25 17:45:31+0800 [scrapy] INFO: Optional features available: ssl, http11
2015-04-25 17:45:31+0800 [scrapy] INFO: Overridden settings: {'DEFAULT_ITEM_CLASS': 'dirbot.items.Website', 'NEWSPIDER_MODULE': 'dirbot.spiders', 'SPIDER_MODULES': ['dirbot.spiders']}
2015-04-25 17:45:31+0800 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-04-25 17:45:31+0800 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-04-25 17:45:31+0800 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-04-25 17:45:31+0800 [scrapy] INFO: Enabled item pipelines: FilterWordsPipeline
2015-04-25 17:45:31+0800 [step1] INFO: Spider opened
2015-04-25 17:45:31+0800 [step1] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-04-25 17:45:31+0800 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6025
2015-04-25 17:45:31+0800 [scrapy] DEBUG: Web service listening on 127.0.0.1:6082
2015-04-25 17:45:31+0800 [step1] DEBUG: Crawled (200) <GET http://bm4.com/bm4.php> (referer: None)
2015-04-25 17:45:31+0800 [step1] DEBUG: Crawled (200) <GET http://bm4.com/bm4.php> (referer: http://bm4.com/bm4.php)
2015-04-25 17:45:31+0800 [step1] INFO: Closing spider (finished)
2015-04-25 17:45:31+0800 [step1] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 499,
	 'downloader/request_count': 2,
	 'downloader/request_method_count/GET': 2,
	 'downloader/response_bytes': 1080,
	 'downloader/response_count': 2,
	 'downloader/response_status_count/200': 2,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 4, 25, 9, 45, 31, 728959),
	 'log_count/DEBUG': 4,
	 'log_count/INFO': 7,
	 'request_depth_max': 1,
	 'response_received_count': 2,
	 'scheduler/dequeued': 2,
	 'scheduler/dequeued/memory': 2,
	 'scheduler/enqueued': 2,
	 'scheduler/enqueued/memory': 2,
	 'start_time': datetime.datetime(2015, 4, 25, 9, 45, 31, 711092)}
2015-04-25 17:45:31+0800 [step1] INFO: Spider closed (finished)
2015-04-25 17:45:32+0800 [scrapy] INFO: Scrapy 0.24.5 started (bot: scrapybot)
2015-04-25 17:45:32+0800 [scrapy] INFO: Optional features available: ssl, http11
2015-04-25 17:45:32+0800 [scrapy] INFO: Overridden settings: {'DEFAULT_ITEM_CLASS': 'dirbot.items.Website', 'NEWSPIDER_MODULE': 'dirbot.spiders', 'SPIDER_MODULES': ['dirbot.spiders']}
2015-04-25 17:45:32+0800 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-04-25 17:45:32+0800 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-04-25 17:45:32+0800 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-04-25 17:45:32+0800 [scrapy] INFO: Enabled item pipelines: FilterWordsPipeline
2015-04-25 17:45:32+0800 [step1login] INFO: Spider opened
2015-04-25 17:45:32+0800 [step1login] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-04-25 17:45:32+0800 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6025
2015-04-25 17:45:32+0800 [scrapy] DEBUG: Web service listening on 127.0.0.1:6082
2015-04-25 17:45:32+0800 [step1login] DEBUG: Crawled (200) <GET http://bm4.com/bm4.php> (referer: None)
2015-04-25 17:45:32+0800 [step1login] ERROR: Spider error processing <GET http://bm4.com/bm4.php>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 26, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/student/Desktop/Code/SqliScanner/Assignment3/dirbot/spiders/step1_login.py", line 35, in parse
	    args, url, method, uid, pid = fill_login_form(response.url, response.body, self.login_user, self.login_pass)
	  File "/home/student/Desktop/Code/SqliScanner/Assignment3/dirbot/spiders/loginform.py", line 80, in fill_login_form
	    form.fields[userfield] = username
	  File "/usr/lib/python2.7/dist-packages/lxml/html/__init__.py", line 960, in __setitem__
	    self.inputs[item].value = value
	  File "/usr/lib/python2.7/dist-packages/lxml/html/__init__.py", line 1025, in __getitem__
	    "No input element with the name %r" % name)
	exceptions.KeyError: 'No input element with the name None'
	
2015-04-25 17:45:32+0800 [step1login] INFO: Closing spider (finished)
2015-04-25 17:45:32+0800 [step1login] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 213,
	 'downloader/request_count': 1,
	 'downloader/request_method_count/GET': 1,
	 'downloader/response_bytes': 566,
	 'downloader/response_count': 1,
	 'downloader/response_status_count/200': 1,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 4, 25, 9, 45, 32, 134355),
	 'log_count/DEBUG': 3,
	 'log_count/ERROR': 1,
	 'log_count/INFO': 7,
	 'response_received_count': 1,
	 'scheduler/dequeued': 1,
	 'scheduler/dequeued/memory': 1,
	 'scheduler/enqueued': 1,
	 'scheduler/enqueued/memory': 1,
	 'spider_exceptions/KeyError': 1,
	 'start_time': datetime.datetime(2015, 4, 25, 9, 45, 32, 122526)}
2015-04-25 17:45:32+0800 [step1login] INFO: Spider closed (finished)
2015-04-25 17:45:32+0800 [scrapy] INFO: Scrapy 0.24.5 started (bot: scrapybot)
2015-04-25 17:45:32+0800 [scrapy] INFO: Optional features available: ssl, http11
2015-04-25 17:45:32+0800 [scrapy] INFO: Overridden settings: {'DEFAULT_ITEM_CLASS': 'dirbot.items.Website', 'NEWSPIDER_MODULE': 'dirbot.spiders', 'SPIDER_MODULES': ['dirbot.spiders']}
2015-04-25 17:45:32+0800 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-04-25 17:45:32+0800 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-04-25 17:45:32+0800 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-04-25 17:45:32+0800 [scrapy] INFO: Enabled item pipelines: FilterWordsPipeline
2015-04-25 17:45:32+0800 [step3login] INFO: Spider opened
2015-04-25 17:45:32+0800 [step3login] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-04-25 17:45:32+0800 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6025
2015-04-25 17:45:32+0800 [scrapy] DEBUG: Web service listening on 127.0.0.1:6082
2015-04-25 17:45:32+0800 [step3login] DEBUG: Crawled (200) <GET http://bm4.com/bm4.php> (referer: None)
2015-04-25 17:45:32+0800 [step3login] DEBUG: nothing to do here
2015-04-25 17:45:32+0800 [step3login] INFO: Closing spider (finished)
2015-04-25 17:45:32+0800 [step3login] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 213,
	 'downloader/request_count': 1,
	 'downloader/request_method_count/GET': 1,
	 'downloader/response_bytes': 566,
	 'downloader/response_count': 1,
	 'downloader/response_status_count/200': 1,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 4, 25, 9, 45, 32, 547351),
	 'log_count/DEBUG': 4,
	 'log_count/INFO': 7,
	 'response_received_count': 1,
	 'scheduler/dequeued': 1,
	 'scheduler/dequeued/memory': 1,
	 'scheduler/enqueued': 1,
	 'scheduler/enqueued/memory': 1,
	 'start_time': datetime.datetime(2015, 4, 25, 9, 45, 32, 535970)}
2015-04-25 17:45:32+0800 [step3login] INFO: Spider closed (finished)
2015-04-25 17:45:32+0800 [scrapy] INFO: Scrapy 0.24.5 started (bot: scrapybot)
2015-04-25 17:45:32+0800 [scrapy] INFO: Optional features available: ssl, http11
2015-04-25 17:45:32+0800 [scrapy] INFO: Overridden settings: {'DEFAULT_ITEM_CLASS': 'dirbot.items.Website', 'NEWSPIDER_MODULE': 'dirbot.spiders', 'SPIDER_MODULES': ['dirbot.spiders']}
2015-04-25 17:45:32+0800 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-04-25 17:45:32+0800 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-04-25 17:45:32+0800 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-04-25 17:45:32+0800 [scrapy] INFO: Enabled item pipelines: FilterWordsPipeline
2015-04-25 17:45:32+0800 [step3] INFO: Spider opened
2015-04-25 17:45:32+0800 [step3] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-04-25 17:45:32+0800 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6025
2015-04-25 17:45:32+0800 [scrapy] DEBUG: Web service listening on 127.0.0.1:6082
2015-04-25 17:45:32+0800 [step3] DEBUG: Crawled (200) <GET http://bm4.com/bm4.php> (referer: None)
2015-04-25 17:45:32+0800 [step3] INFO: Closing spider (finished)
2015-04-25 17:45:32+0800 [step3] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 213,
	 'downloader/request_count': 1,
	 'downloader/request_method_count/GET': 1,
	 'downloader/response_bytes': 566,
	 'downloader/response_count': 1,
	 'downloader/response_status_count/200': 1,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 4, 25, 9, 45, 32, 967094),
	 'log_count/DEBUG': 3,
	 'log_count/INFO': 7,
	 'response_received_count': 1,
	 'scheduler/dequeued': 1,
	 'scheduler/dequeued/memory': 1,
	 'scheduler/enqueued': 1,
	 'scheduler/enqueued/memory': 1,
	 'start_time': datetime.datetime(2015, 4, 25, 9, 45, 32, 957052)}
2015-04-25 17:45:32+0800 [step3] INFO: Spider closed (finished)
2015-04-25 17:45:33+0800 [scrapy] INFO: Scrapy 0.24.5 started (bot: scrapybot)
2015-04-25 17:45:33+0800 [scrapy] INFO: Optional features available: ssl, http11
2015-04-25 17:45:33+0800 [scrapy] INFO: Overridden settings: {'DEFAULT_ITEM_CLASS': 'dirbot.items.Website', 'NEWSPIDER_MODULE': 'dirbot.spiders', 'SPIDER_MODULES': ['dirbot.spiders']}
2015-04-25 17:45:33+0800 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-04-25 17:45:33+0800 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-04-25 17:45:33+0800 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-04-25 17:45:33+0800 [scrapy] INFO: Enabled item pipelines: FilterWordsPipeline
2015-04-25 17:45:33+0800 [step1] INFO: Spider opened
2015-04-25 17:45:33+0800 [step1] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-04-25 17:45:33+0800 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6025
2015-04-25 17:45:33+0800 [scrapy] DEBUG: Web service listening on 127.0.0.1:6082
2015-04-25 17:45:33+0800 [step1] DEBUG: Crawled (200) <GET https://bm2.com/bm2/login.php> (referer: None)
2015-04-25 17:45:33+0800 [step1] DEBUG: Crawled (200) <GET https://bm2.com/bm2/login.php> (referer: https://bm2.com/bm2/login.php)
2015-04-25 17:45:33+0800 [step1] DEBUG: Crawled (404) <GET https://bm2.com/bm2/formhash(this.form,%20this.form.password)> (referer: https://bm2.com/bm2/login.php)
2015-04-25 17:45:33+0800 [step1] DEBUG: Ignoring response <404 https://bm2.com/bm2/formhash(this.form,%20this.form.password)>: HTTP status code is not handled or not allowed
2015-04-25 17:45:33+0800 [step1] DEBUG: Crawled (404) <GET https://bm2.com/bm2/register.php> (referer: https://bm2.com/bm2/login.php)
2015-04-25 17:45:33+0800 [step1] DEBUG: Ignoring response <404 https://bm2.com/bm2/register.php>: HTTP status code is not handled or not allowed
2015-04-25 17:45:33+0800 [step1] INFO: Closing spider (finished)
2015-04-25 17:45:33+0800 [step1] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 1190,
	 'downloader/request_count': 4,
	 'downloader/request_method_count/GET': 4,
	 'downloader/response_bytes': 2592,
	 'downloader/response_count': 4,
	 'downloader/response_status_count/200': 2,
	 'downloader/response_status_count/404': 2,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 4, 25, 9, 45, 33, 402589),
	 'log_count/DEBUG': 8,
	 'log_count/INFO': 7,
	 'request_depth_max': 2,
	 'response_received_count': 4,
	 'scheduler/dequeued': 4,
	 'scheduler/dequeued/memory': 4,
	 'scheduler/enqueued': 4,
	 'scheduler/enqueued/memory': 4,
	 'start_time': datetime.datetime(2015, 4, 25, 9, 45, 33, 365479)}
2015-04-25 17:45:33+0800 [step1] INFO: Spider closed (finished)
2015-04-25 17:45:33+0800 [scrapy] INFO: Scrapy 0.24.5 started (bot: scrapybot)
2015-04-25 17:45:33+0800 [scrapy] INFO: Optional features available: ssl, http11
2015-04-25 17:45:33+0800 [scrapy] INFO: Overridden settings: {'DEFAULT_ITEM_CLASS': 'dirbot.items.Website', 'NEWSPIDER_MODULE': 'dirbot.spiders', 'SPIDER_MODULES': ['dirbot.spiders']}
2015-04-25 17:45:33+0800 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-04-25 17:45:33+0800 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-04-25 17:45:33+0800 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-04-25 17:45:33+0800 [scrapy] INFO: Enabled item pipelines: FilterWordsPipeline
2015-04-25 17:45:33+0800 [step1login] INFO: Spider opened
2015-04-25 17:45:33+0800 [step1login] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-04-25 17:45:33+0800 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6025
2015-04-25 17:45:33+0800 [scrapy] DEBUG: Web service listening on 127.0.0.1:6082
2015-04-25 17:45:33+0800 [step1login] DEBUG: Crawled (200) <GET https://bm2.com/bm2/login.php> (referer: None)
2015-04-25 17:45:33+0800 [step1login] DEBUG: Crawled (200) <POST https://bm2.com/bm2/login.php> (referer: https://bm2.com/bm2/login.php)
2015-04-25 17:45:33+0800 [step1login] ERROR: Login failed
2015-04-25 17:45:33+0800 [step1login] INFO: Closing spider (finished)
2015-04-25 17:45:33+0800 [step1login] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 619,
	 'downloader/request_count': 2,
	 'downloader/request_method_count/GET': 1,
	 'downloader/request_method_count/POST': 1,
	 'downloader/response_bytes': 1713,
	 'downloader/response_count': 2,
	 'downloader/response_status_count/200': 2,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 4, 25, 9, 45, 33, 829789),
	 'log_count/DEBUG': 4,
	 'log_count/ERROR': 1,
	 'log_count/INFO': 7,
	 'request_depth_max': 1,
	 'response_received_count': 2,
	 'scheduler/dequeued': 2,
	 'scheduler/dequeued/memory': 2,
	 'scheduler/enqueued': 2,
	 'scheduler/enqueued/memory': 2,
	 'start_time': datetime.datetime(2015, 4, 25, 9, 45, 33, 807432)}
2015-04-25 17:45:33+0800 [step1login] INFO: Spider closed (finished)
2015-04-25 17:45:34+0800 [scrapy] INFO: Scrapy 0.24.5 started (bot: scrapybot)
2015-04-25 17:45:34+0800 [scrapy] INFO: Optional features available: ssl, http11
2015-04-25 17:45:34+0800 [scrapy] INFO: Overridden settings: {'DEFAULT_ITEM_CLASS': 'dirbot.items.Website', 'NEWSPIDER_MODULE': 'dirbot.spiders', 'SPIDER_MODULES': ['dirbot.spiders']}
2015-04-25 17:45:34+0800 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-04-25 17:45:34+0800 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-04-25 17:45:34+0800 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-04-25 17:45:34+0800 [scrapy] INFO: Enabled item pipelines: FilterWordsPipeline
2015-04-25 17:45:34+0800 [step3login] INFO: Spider opened
2015-04-25 17:45:34+0800 [step3login] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-04-25 17:45:34+0800 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6025
2015-04-25 17:45:34+0800 [scrapy] DEBUG: Web service listening on 127.0.0.1:6082
2015-04-25 17:45:34+0800 [step3login] DEBUG: Crawled (200) <GET https://bm2.com/bm2/login.php> (referer: None)
2015-04-25 17:45:34+0800 [step3login] DEBUG: Crawled (200) <POST https://bm2.com/bm2/includes/process_login.php> (referer: https://bm2.com/bm2/login.php)
2015-04-25 17:45:34+0800 [step3login] INFO: Closing spider (finished)
2015-04-25 17:45:34+0800 [step3login] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 642,
	 'downloader/request_count': 2,
	 'downloader/request_method_count/GET': 1,
	 'downloader/request_method_count/POST': 1,
	 'downloader/response_bytes': 1269,
	 'downloader/response_count': 2,
	 'downloader/response_status_count/200': 2,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 4, 25, 9, 45, 34, 239444),
	 'log_count/DEBUG': 4,
	 'log_count/INFO': 7,
	 'request_depth_max': 1,
	 'response_received_count': 2,
	 'scheduler/dequeued': 2,
	 'scheduler/dequeued/memory': 2,
	 'scheduler/enqueued': 2,
	 'scheduler/enqueued/memory': 2,
	 'start_time': datetime.datetime(2015, 4, 25, 9, 45, 34, 218986)}
2015-04-25 17:45:34+0800 [step3login] INFO: Spider closed (finished)
2015-04-25 17:45:34+0800 [scrapy] INFO: Scrapy 0.24.5 started (bot: scrapybot)
2015-04-25 17:45:34+0800 [scrapy] INFO: Optional features available: ssl, http11
2015-04-25 17:45:34+0800 [scrapy] INFO: Overridden settings: {'DEFAULT_ITEM_CLASS': 'dirbot.items.Website', 'NEWSPIDER_MODULE': 'dirbot.spiders', 'SPIDER_MODULES': ['dirbot.spiders']}
2015-04-25 17:45:34+0800 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-04-25 17:45:34+0800 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-04-25 17:45:34+0800 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-04-25 17:45:34+0800 [scrapy] INFO: Enabled item pipelines: FilterWordsPipeline
2015-04-25 17:45:34+0800 [step3] INFO: Spider opened
2015-04-25 17:45:34+0800 [step3] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-04-25 17:45:34+0800 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6025
2015-04-25 17:45:34+0800 [scrapy] DEBUG: Web service listening on 127.0.0.1:6082
2015-04-25 17:45:34+0800 [step3] DEBUG: Crawled (200) <GET https://bm2.com/bm2/login.php> (referer: None)
2015-04-25 17:45:34+0800 [step3] DEBUG: Crawled (200) <POST https://bm2.com/bm2/includes/process_login.php> (referer: https://bm2.com/bm2/login.php)
2015-04-25 17:45:34+0800 [step3] DEBUG: Crawled (404) <GET https://bm2.com/bm2/formhash(this.form,%20this.form.password)> (referer: https://bm2.com/bm2/includes/process_login.php)
2015-04-25 17:45:34+0800 [step3] DEBUG: Ignoring response <404 https://bm2.com/bm2/formhash(this.form,%20this.form.password)>: HTTP status code is not handled or not allowed
2015-04-25 17:45:34+0800 [step3] DEBUG: Crawled (404) <GET https://bm2.com/bm2/register.php> (referer: https://bm2.com/bm2/includes/process_login.php)
2015-04-25 17:45:34+0800 [step3] DEBUG: Ignoring response <404 https://bm2.com/bm2/register.php>: HTTP status code is not handled or not allowed
2015-04-25 17:45:34+0800 [step3] INFO: Closing spider (finished)
2015-04-25 17:45:34+0800 [step3] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 1329,
	 'downloader/request_count': 4,
	 'downloader/request_method_count/GET': 3,
	 'downloader/request_method_count/POST': 1,
	 'downloader/response_bytes': 2148,
	 'downloader/response_count': 4,
	 'downloader/response_status_count/200': 2,
	 'downloader/response_status_count/404': 2,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 4, 25, 9, 45, 34, 683080),
	 'log_count/DEBUG': 8,
	 'log_count/INFO': 7,
	 'request_depth_max': 2,
	 'response_received_count': 4,
	 'scheduler/dequeued': 4,
	 'scheduler/dequeued/memory': 4,
	 'scheduler/enqueued': 4,
	 'scheduler/enqueued/memory': 4,
	 'start_time': datetime.datetime(2015, 4, 25, 9, 45, 34, 646144)}
2015-04-25 17:45:34+0800 [step3] INFO: Spider closed (finished)
